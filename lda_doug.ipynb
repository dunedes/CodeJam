{
 "metadata": {
  "name": "",
  "signature": "sha256:c538fb51cb273bc5cb3bad024b3dd73ac6243ddf4ba03ce7c62e7498faf7c7aa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import glob\n",
      "import os\n",
      "from PIL import Image\n",
      "from face_rec.utils import make_array\n",
      "from face_rec.utils import get_IDs\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from face_rec.pre_process import FishTrainer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from face_rec.recognize import FishRecon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trained_names = glob.glob('training_dataset_cropped2/*.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = Image.open(trained_names[0]).convert(\"L\")\n",
      "H,W = np.shape(im)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr = np.zeros([len(trained_names), H * W])\n",
      "for index,filename in enumerate(trained_names):\n",
      "    # im = Image.open(filename).convert(\"L\")\n",
      "    # arr[i,:] = np.reshape(np.asarray(im),[1,H*W])\n",
      "    arr[index,:] = make_array(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IDs = get_IDs(trained_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fishtrain = FishTrainer(arr,IDs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fishtrain.save('bin/og/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recon = FishRecon('bin/og/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files = glob.glob('training_dataset_cropped2/*.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(all_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "120"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_d = {}\n",
      "a_list = []\n",
      "fits = []\n",
      "for a_file in all_files:\n",
      "    a_picture = make_array(a_file)\n",
      "    predicted_lda,fit = recon.recognize(a_picture)\n",
      "    fits.append(fit)\n",
      "    a_list.append((fit,a_file))\n",
      "    if fit>0:\n",
      "        my_d[a_file] = (predicted_lda,fit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "[(28.445239156977628, 'training_dataset_cropped2\\\\10_11__cropped.png'),\n",
        " (1.9781306678061839, 'training_dataset_cropped2\\\\10_1__cropped.png'),\n",
        " (16.233683780940076, 'training_dataset_cropped2\\\\10_2__cropped.png'),\n",
        " (20.87226679427954, 'training_dataset_cropped2\\\\10_4__cropped.png'),\n",
        " (36.253248020606343, 'training_dataset_cropped2\\\\10_5__cropped.png'),\n",
        " (17.691297882419079, 'training_dataset_cropped2\\\\10_6__cropped.png'),\n",
        " (45.850683539414725, 'training_dataset_cropped2\\\\10_8__cropped.png'),\n",
        " (46.815616292826796, 'training_dataset_cropped2\\\\10_9__cropped.png'),\n",
        " (70.664368895902612, 'training_dataset_cropped2\\\\11_10__cropped.png'),\n",
        " (73.713774071221223, 'training_dataset_cropped2\\\\11_1__cropped.png'),\n",
        " (81.803944037083454, 'training_dataset_cropped2\\\\11_2__cropped.png'),\n",
        " (95.505412538843984, 'training_dataset_cropped2\\\\11_3__cropped.png'),\n",
        " (35.82322389205919, 'training_dataset_cropped2\\\\11_4__cropped.png'),\n",
        " (90.511960862620299, 'training_dataset_cropped2\\\\11_5__cropped.png'),\n",
        " (85.290867250514538, 'training_dataset_cropped2\\\\11_6__cropped.png'),\n",
        " (74.363449319542184, 'training_dataset_cropped2\\\\11_8__cropped.png'),\n",
        " (60.988458203060219, 'training_dataset_cropped2\\\\12_10__cropped.png'),\n",
        " (78.472043945326703, 'training_dataset_cropped2\\\\12_11__cropped.png'),\n",
        " (44.384242227715561, 'training_dataset_cropped2\\\\12_1__cropped.png'),\n",
        " (80.33727372375435, 'training_dataset_cropped2\\\\12_2__cropped.png'),\n",
        " (45.040136172345754, 'training_dataset_cropped2\\\\12_5__cropped.png'),\n",
        " (66.63384976193683, 'training_dataset_cropped2\\\\12_7__cropped.png'),\n",
        " (67.868522902199089, 'training_dataset_cropped2\\\\12_8__cropped.png'),\n",
        " (70.813377140819426, 'training_dataset_cropped2\\\\12_9__cropped.png'),\n",
        " (54.793960690749891, 'training_dataset_cropped2\\\\13_10__cropped.png'),\n",
        " (91.164460727668853, 'training_dataset_cropped2\\\\13_1__cropped.png'),\n",
        " (79.211915164687355, 'training_dataset_cropped2\\\\13_2__cropped.png'),\n",
        " (57.78242433377698, 'training_dataset_cropped2\\\\13_3__cropped.png'),\n",
        " (17.169504580858224, 'training_dataset_cropped2\\\\13_4__cropped.png'),\n",
        " (52.737389689746799, 'training_dataset_cropped2\\\\13_6__cropped.png'),\n",
        " (5.2914999056262033, 'training_dataset_cropped2\\\\13_7__cropped.png'),\n",
        " (52.512465179182961, 'training_dataset_cropped2\\\\13_9__cropped.png'),\n",
        " (13.803680581169953, 'training_dataset_cropped2\\\\14_10__cropped.png'),\n",
        " (15.51813646147821, 'training_dataset_cropped2\\\\14_1__cropped.png'),\n",
        " (22.609629718282026, 'training_dataset_cropped2\\\\14_2__cropped.png'),\n",
        " (35.880550375638464, 'training_dataset_cropped2\\\\14_3__cropped.png'),\n",
        " (25.626405972126502, 'training_dataset_cropped2\\\\14_5__cropped.png'),\n",
        " (25.626405972126502, 'training_dataset_cropped2\\\\14_6__cropped.png'),\n",
        " (-0.92652283173751471, 'training_dataset_cropped2\\\\14_7__cropped.png'),\n",
        " (4.2055139685917204, 'training_dataset_cropped2\\\\14_8__cropped.png'),\n",
        " (23.110334639271315, 'training_dataset_cropped2\\\\15_10__cropped.png'),\n",
        " (26.848758006896773, 'training_dataset_cropped2\\\\15_11__cropped.png'),\n",
        " (-0.28198072039114663, 'training_dataset_cropped2\\\\15_1__cropped.png'),\n",
        " (44.926140185813523, 'training_dataset_cropped2\\\\15_3__cropped.png'),\n",
        " (20.325360650904003, 'training_dataset_cropped2\\\\15_5__cropped.png'),\n",
        " (3.1870656419557761, 'training_dataset_cropped2\\\\15_7__cropped.png'),\n",
        " (41.976047036623058, 'training_dataset_cropped2\\\\15_8__cropped.png'),\n",
        " (57.805455130820491, 'training_dataset_cropped2\\\\15_9__cropped.png'),\n",
        " (36.4656930912858, 'training_dataset_cropped2\\\\1_10__cropped.png'),\n",
        " (42.441652973051617, 'training_dataset_cropped2\\\\1_11__cropped.png'),\n",
        " (35.221515160419585, 'training_dataset_cropped2\\\\1_2__cropped.png'),\n",
        " (7.4955696926987656, 'training_dataset_cropped2\\\\1_4__cropped.png'),\n",
        " (51.768591200944329, 'training_dataset_cropped2\\\\1_5__cropped.png'),\n",
        " (46.477531480056513, 'training_dataset_cropped2\\\\1_6__cropped.png'),\n",
        " (-11.013232628857295, 'training_dataset_cropped2\\\\1_7__cropped.png'),\n",
        " (40.969151847021394, 'training_dataset_cropped2\\\\1_9__cropped.png'),\n",
        " (27.944576042447714, 'training_dataset_cropped2\\\\2_11__cropped.png'),\n",
        " (23.47658247164312, 'training_dataset_cropped2\\\\2_1__cropped.png'),\n",
        " (43.983000722340925, 'training_dataset_cropped2\\\\2_2__cropped.png'),\n",
        " (34.000030801480371, 'training_dataset_cropped2\\\\2_3__cropped.png'),\n",
        " (57.371787780928834, 'training_dataset_cropped2\\\\2_5__cropped.png'),\n",
        " (57.371787780928834, 'training_dataset_cropped2\\\\2_6__cropped.png'),\n",
        " (29.881602071986201, 'training_dataset_cropped2\\\\2_8__cropped.png'),\n",
        " (25.368377303587017, 'training_dataset_cropped2\\\\2_9__cropped.png'),\n",
        " (38.757048753710286, 'training_dataset_cropped2\\\\3_11__cropped.png'),\n",
        " (28.440609662973664, 'training_dataset_cropped2\\\\3_1__cropped.png'),\n",
        " (58.093536124070788, 'training_dataset_cropped2\\\\3_3__cropped.png'),\n",
        " (0.96740772931725871, 'training_dataset_cropped2\\\\3_4__cropped.png'),\n",
        " (44.58453715680313, 'training_dataset_cropped2\\\\3_5__cropped.png'),\n",
        " (44.58453715680313, 'training_dataset_cropped2\\\\3_6__cropped.png'),\n",
        " (3.8919041931717544, 'training_dataset_cropped2\\\\3_7__cropped.png'),\n",
        " (61.114510847807871, 'training_dataset_cropped2\\\\3_9__cropped.png'),\n",
        " (38.847368101711012, 'training_dataset_cropped2\\\\4_10__cropped.png'),\n",
        " (15.596799455499671, 'training_dataset_cropped2\\\\4_11__cropped.png'),\n",
        " (8.2827838990578826, 'training_dataset_cropped2\\\\4_1__cropped.png'),\n",
        " (31.972351894682816, 'training_dataset_cropped2\\\\4_3__cropped.png'),\n",
        " (52.4661986007103, 'training_dataset_cropped2\\\\4_5__cropped.png'),\n",
        " (41.881411297319417, 'training_dataset_cropped2\\\\4_6__cropped.png'),\n",
        " (41.881411297319417, 'training_dataset_cropped2\\\\4_8__cropped.png'),\n",
        " (44.587840589763587, 'training_dataset_cropped2\\\\4_9__cropped.png'),\n",
        " (105.59924953852504, 'training_dataset_cropped2\\\\5_10__cropped.png'),\n",
        " (33.438405998455153, 'training_dataset_cropped2\\\\5_1__cropped.png'),\n",
        " (115.9609181569829, 'training_dataset_cropped2\\\\5_3__cropped.png'),\n",
        " (160.12714174458296, 'training_dataset_cropped2\\\\5_5__cropped.png'),\n",
        " (115.40354590229992, 'training_dataset_cropped2\\\\5_6__cropped.png'),\n",
        " (48.016055068266496, 'training_dataset_cropped2\\\\5_7__cropped.png'),\n",
        " (123.55668003849169, 'training_dataset_cropped2\\\\5_8__cropped.png'),\n",
        " (122.76971433424873, 'training_dataset_cropped2\\\\5_9__cropped.png'),\n",
        " (17.31843938935884, 'training_dataset_cropped2\\\\6_11__cropped.png'),\n",
        " (39.704589229108848, 'training_dataset_cropped2\\\\6_2__cropped.png'),\n",
        " (23.305884525141057, 'training_dataset_cropped2\\\\6_3__cropped.png'),\n",
        " (13.239196430735642, 'training_dataset_cropped2\\\\6_4__cropped.png'),\n",
        " (15.409164015922837, 'training_dataset_cropped2\\\\6_5__cropped.png'),\n",
        " (16.081811827360571, 'training_dataset_cropped2\\\\6_7__cropped.png'),\n",
        " (6.0196718263384845, 'training_dataset_cropped2\\\\6_8__cropped.png'),\n",
        " (12.384989757900751, 'training_dataset_cropped2\\\\6_9__cropped.png'),\n",
        " (33.987800101561589, 'training_dataset_cropped2\\\\7_10__cropped.png'),\n",
        " (29.176403209236966, 'training_dataset_cropped2\\\\7_11__cropped.png'),\n",
        " (7.6062971465716558, 'training_dataset_cropped2\\\\7_1__cropped.png'),\n",
        " (31.937891707201089, 'training_dataset_cropped2\\\\7_2__cropped.png'),\n",
        " (40.578598052773827, 'training_dataset_cropped2\\\\7_3__cropped.png'),\n",
        " (13.276236697386842, 'training_dataset_cropped2\\\\7_4__cropped.png'),\n",
        " (22.617084149637378, 'training_dataset_cropped2\\\\7_6__cropped.png'),\n",
        " (37.223920438390195, 'training_dataset_cropped2\\\\7_8__cropped.png'),\n",
        " (40.787034138091762, 'training_dataset_cropped2\\\\8_11__cropped.png'),\n",
        " (19.3777503721412, 'training_dataset_cropped2\\\\8_1__cropped.png'),\n",
        " (33.6076919140557, 'training_dataset_cropped2\\\\8_2__cropped.png'),\n",
        " (23.474650399080446, 'training_dataset_cropped2\\\\8_4__cropped.png'),\n",
        " (25.895004727905771, 'training_dataset_cropped2\\\\8_5__cropped.png'),\n",
        " (14.729566683870651, 'training_dataset_cropped2\\\\8_7__cropped.png'),\n",
        " (33.609264041943369, 'training_dataset_cropped2\\\\8_8__cropped.png'),\n",
        " (41.789507342502837, 'training_dataset_cropped2\\\\8_9__cropped.png'),\n",
        " (66.984600547477129, 'training_dataset_cropped2\\\\9_10__cropped.png'),\n",
        " (65.97447178426097, 'training_dataset_cropped2\\\\9_11__cropped.png'),\n",
        " (61.977473202257784, 'training_dataset_cropped2\\\\9_2__cropped.png'),\n",
        " (77.499581163745091, 'training_dataset_cropped2\\\\9_5__cropped.png'),\n",
        " (77.499581163745091, 'training_dataset_cropped2\\\\9_6__cropped.png'),\n",
        " (35.742187929384329, 'training_dataset_cropped2\\\\9_7__cropped.png'),\n",
        " (42.612796779739526, 'training_dataset_cropped2\\\\9_8__cropped.png'),\n",
        " (87.270639320325273, 'training_dataset_cropped2\\\\9_9__cropped.png')]"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np_fits = np.asarray(fits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files[np_fits.argmax()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "'doug_all/doug_test1\\\\doug_260396387_2_920499.png'"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l[:,0].max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "cannot perform reduce with flexible type",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-35dc92762271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Python27\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     return um.maximum.reduce(a, axis=axis,\n\u001b[1;32m---> 17\u001b[1;33m                             out=out, keepdims=keepdims)\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxidx = l[:,0].argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l[maxidx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "array(['-0.89193065198',\n",
        "       'doug_all/doug_test1\\\\doug_260396387_2_9289154.png'], \n",
        "      dtype='|S48')"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[(8, 2), (14, 4), (6, 10), (8, 15), (6, 15)]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits = np.asarray(fits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits[fits<20].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "(14,)"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_target.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "255"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_prob = clf.predict_proba(make_array(all_files[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([[  1.00000000e+00,   6.61838216e-46,   2.74725801e-38,\n",
        "          3.99919660e-42,   4.05870007e-41,   1.04088164e-26,\n",
        "          2.94991646e-38,   3.32041409e-33,   3.46870257e-61,\n",
        "          7.99012944e-40,   3.05495310e-34,   9.08117121e-33,\n",
        "          2.18125380e-53,   3.32008579e-33,   4.84449394e-34]])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_target = make_array(all_files[20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# target_2d = np.asarray(np.atleast_2d(a_target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.dot(a_target - mean,scalings)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = np.dot(X, coeffs)+intercept"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "array([ -58.48831635,   27.94457604,  -59.93175328,  -37.66377309,\n",
        "       -169.86479197,   -5.86840073,  -11.48182364,  -24.32087449,\n",
        "        -28.47509457,  -22.70443046, -147.37975159,  -94.0779447 ,\n",
        "        -67.4654941 ,  -19.82000088,   -4.89433692])"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best = res.argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_best(target_face):\n",
      "    X = np.dot(target_face - mean,scalings)\n",
      "    res = np.dot(X, coeffs)+intercept\n",
      "    best = res.argmax()\n",
      "    return classes[best]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classes.take(best)\n",
      "print classes[best]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "2\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict_log_proba(make_array(all_files[5])).argmin()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "8"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files[15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "'fullDataSetB_cropped2\\\\02_normal_cropped.png'"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "[(8, 2), (14, 4), (6, 10), (8, 15), (6, 15)]"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = clf.predict(make_array('fullDataSetB_cropped2/04_centerlight_cropped.png'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "int(predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_image = np.load('bin/eigenfaces/mean_image.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def image_grid(D,H,W,cols=10,scale=1):\n",
      "    \"\"\" display a grid of images\n",
      "        H,W: Height and width of the images\n",
      "        cols: number of columns = number of images in each row\n",
      "        scale: 1 to fill screen\n",
      "    \"\"\"\n",
      "    n = np.shape(D)[0]\n",
      "    rows = int(math.ceil((n+0.0)/cols))\n",
      "    fig = plt.figure(1,figsize=[scale*20.0/H*W,scale*20.0/cols*rows],dpi=300)\n",
      "    for i in range(n):\n",
      "        plt.subplot(rows,cols,i+1)\n",
      "        fig=plt.imshow(np.reshape(D[i,:],[H,W]), cmap = plt.get_cmap(\"gray\"))\n",
      "        plt.axis('off')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}