{
 "metadata": {
  "name": "",
  "signature": "sha256:01a34ed1eab3a0d5bd1d45b402e95767ef2f2fa929807a85f60c57d422b49226"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import glob\n",
      "import os\n",
      "from PIL import Image\n",
      "from face_rec.utils import make_array\n",
      "from face_rec.utils import get_IDs\n",
      "import math\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from face_rec.pre_process import EigenFacialTrainer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from face_rec.recognize import EigenfaceRecon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trained_names = glob.glob('doug_trained/*.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im = Image.open(trained_names[0]).convert(\"L\")\n",
      "H,W = np.shape(im)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arr = np.zeros([len(trained_names), H * W])\n",
      "for index,filename in enumerate(trained_names):\n",
      "    # im = Image.open(filename).convert(\"L\")\n",
      "    # arr[i,:] = np.reshape(np.asarray(im),[1,H*W])\n",
      "    arr[index,:] = make_array(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IDs = get_IDs(trained_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = EigenFacialTrainer(arr,IDs)\n",
      "train.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train.save('bin/eigen_og/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "['eigenfaces', 'mean_image', 'labels', 'scores']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recon = EigenfaceRecon('bin/eigen_og/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files = glob.glob('training_dataset_cropped2/*.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recon.recognize(make_array('training_dataset_cropped2/10_4__cropped.png'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(10, 2.8803471236272084e-11)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_d = {}\n",
      "a_list = []\n",
      "fits = []\n",
      "for a_file in all_files:\n",
      "    a_picture = make_array(a_file)\n",
      "    predicted_lda,fit = recon.recognize(a_picture)\n",
      "    print predicted_lda,fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 4.93918107813e-11\n",
        "10 3.3820834021e-11\n",
        "10 2.22468210983e-11\n",
        "10 3.05014142318e-11\n",
        "10 2.19563270133e-11\n",
        "10 3.04523617302e-11\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.87168365989e-11\n",
        "10 3.03282844486e-11\n",
        "11 6.02914176501e-11\n",
        "11 5.14710315157e-11\n",
        "11 4.11238624025e-11\n",
        "11 3.01327353823e-11\n",
        "11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.74186880602e-11\n",
        "11 4.78129571728e-11\n",
        "11 6.26814299961e-11\n",
        "11 3.32286426045e-11\n",
        "12 4.18972546623e-11\n",
        "12 2.96886949921e-11\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.0314390724e-11\n",
        "12 4.20962986602e-11\n",
        "12 2.91856591431e-11\n",
        "12 3.60459408885e-11\n",
        "12 3.80703262995e-11\n",
        "12 4.73213607737e-11\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.75577515753e-11\n",
        "13 3.36608081415e-11\n",
        "13 4.07818745084e-11\n",
        "13 3.96392024084e-11\n",
        "13 0.0\n",
        "13 5.00222085975e-12\n",
        "13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.91576687457e-11\n",
        "13 4.64010776356e-11\n",
        "14 2.26458072343e-11\n",
        "14 2.13146039021e-11\n",
        "14 1.53681047407e-11\n",
        "14 3.01437805768e-11\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.14187686772e-11\n",
        "14 2.14187686772e-11\n",
        "14 3.18906930168e-11\n",
        "14 2.41871119782e-11\n",
        "15 4.18196928247e-11\n",
        "15 2.79918471371e-11\n",
        "15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.47683584654e-11\n",
        "15 4.37965191148e-11\n",
        "15 4.86372659087e-11\n",
        "15 4.63916359242e-11\n",
        "15 4.69104166027e-11\n",
        "15 4.17474089248e-11\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.00661228139e-11\n",
        "1 4.70386259801e-11\n",
        "1 3.18006622602e-11\n",
        "1 3.15636102222e-11\n",
        "1 4.00272413465e-11\n",
        "1 3.10276629757e-11\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.13858575057e-11\n",
        "1 6.49731431625e-11\n",
        "2 2.90497481221e-11\n",
        "2 3.90557633994e-11\n",
        "2 0.0\n",
        "2 8.18545231596e-12\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.93630939524e-11\n",
        "2 2.93630939524e-11\n",
        "2 3.24309531349e-11\n",
        "2 3.18083952333e-11\n",
        "3 3.8394920511e-11\n",
        "3 3.7158144773e-11\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.15061963747e-11\n",
        "3 3.25518771262e-11\n",
        "3 3.80155578702e-11\n",
        "3 3.74483116239e-11\n",
        "3 5.83289444912e-11\n",
        "3 2.85214225918e-11\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.96652160697e-11\n",
        "4 1.8185742339e-11\n",
        "4 2.41272507887e-11\n",
        "4 2.41850449655e-11\n",
        "4 1.98845020022e-11\n",
        "4 3.25891204872e-11\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.57743772886e-11\n",
        "4 2.43325927381e-11\n",
        "5 3.64777696608e-11\n",
        "5 3.6313541947e-11\n",
        "5 6.35567498361e-11\n",
        "5 2.92100906363e-11\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.83861408434e-11\n",
        "5 3.05161570212e-11\n",
        "5 3.0596927531e-11\n",
        "5 2.93888971207e-11\n",
        "6 0.0\n",
        "6 1.02318153949e-12\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.81712860028e-11\n",
        "6 3.44774885167e-11\n",
        "6 3.97098700389e-11\n",
        "6 2.86732727652e-11\n",
        "6 2.27436083714e-11\n",
        "6 2.54629343451e-11\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.08729070891e-11\n",
        "7 3.08031595548e-11\n",
        "7 4.20364191152e-11\n",
        "7 2.41448040117e-11\n",
        "7 3.77904097851e-11\n",
        "7 3.24742280502e-11\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.97556916021e-11\n",
        "7 4.81556940247e-11\n",
        "8 2.75625577685e-11\n",
        "8 2.45497328442e-11\n",
        "8 4.41078159558e-11\n",
        "8 3.07265935559e-11\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.43001481178e-11\n",
        "8 3.04396259998e-11\n",
        "8 4.71343748333e-11\n",
        "8 4.64452669662e-11\n",
        "9 6.14725773186e-11\n",
        "9 8.03582134045e-11\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.90772284118e-11\n",
        "9 5.02464619278e-11\n",
        "9 4.48260350092e-11\n",
        "9 3.85664751847e-11\n",
        "9 0.0\n",
        "9 6.36646291241e-12\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(0, 'training_dataset_cropped2\\\\10_11__cropped.png'),\n",
        " (1, 'training_dataset_cropped2\\\\10_1__cropped.png'),\n",
        " (2, 'training_dataset_cropped2\\\\10_2__cropped.png'),\n",
        " (3, 'training_dataset_cropped2\\\\10_4__cropped.png'),\n",
        " (4, 'training_dataset_cropped2\\\\10_5__cropped.png'),\n",
        " (5, 'training_dataset_cropped2\\\\10_6__cropped.png'),\n",
        " (6, 'training_dataset_cropped2\\\\10_8__cropped.png'),\n",
        " (7, 'training_dataset_cropped2\\\\10_9__cropped.png'),\n",
        " (8, 'training_dataset_cropped2\\\\11_10__cropped.png'),\n",
        " (9, 'training_dataset_cropped2\\\\11_1__cropped.png'),\n",
        " (10, 'training_dataset_cropped2\\\\11_2__cropped.png'),\n",
        " (11, 'training_dataset_cropped2\\\\11_3__cropped.png'),\n",
        " (12, 'training_dataset_cropped2\\\\11_4__cropped.png'),\n",
        " (13, 'training_dataset_cropped2\\\\11_5__cropped.png'),\n",
        " (14, 'training_dataset_cropped2\\\\11_6__cropped.png'),\n",
        " (15, 'training_dataset_cropped2\\\\11_8__cropped.png'),\n",
        " (16, 'training_dataset_cropped2\\\\12_10__cropped.png'),\n",
        " (17, 'training_dataset_cropped2\\\\12_11__cropped.png'),\n",
        " (18, 'training_dataset_cropped2\\\\12_1__cropped.png'),\n",
        " (19, 'training_dataset_cropped2\\\\12_2__cropped.png'),\n",
        " (20, 'training_dataset_cropped2\\\\12_5__cropped.png'),\n",
        " (21, 'training_dataset_cropped2\\\\12_7__cropped.png'),\n",
        " (22, 'training_dataset_cropped2\\\\12_8__cropped.png'),\n",
        " (23, 'training_dataset_cropped2\\\\12_9__cropped.png'),\n",
        " (24, 'training_dataset_cropped2\\\\13_10__cropped.png'),\n",
        " (25, 'training_dataset_cropped2\\\\13_1__cropped.png'),\n",
        " (26, 'training_dataset_cropped2\\\\13_2__cropped.png'),\n",
        " (27, 'training_dataset_cropped2\\\\13_3__cropped.png'),\n",
        " (28, 'training_dataset_cropped2\\\\13_4__cropped.png'),\n",
        " (29, 'training_dataset_cropped2\\\\13_6__cropped.png'),\n",
        " (30, 'training_dataset_cropped2\\\\13_7__cropped.png'),\n",
        " (31, 'training_dataset_cropped2\\\\13_9__cropped.png'),\n",
        " (32, 'training_dataset_cropped2\\\\14_10__cropped.png'),\n",
        " (33, 'training_dataset_cropped2\\\\14_1__cropped.png'),\n",
        " (34, 'training_dataset_cropped2\\\\14_2__cropped.png'),\n",
        " (35, 'training_dataset_cropped2\\\\14_3__cropped.png'),\n",
        " (36, 'training_dataset_cropped2\\\\14_5__cropped.png'),\n",
        " (36, 'training_dataset_cropped2\\\\14_6__cropped.png'),\n",
        " (38, 'training_dataset_cropped2\\\\14_7__cropped.png'),\n",
        " (39, 'training_dataset_cropped2\\\\14_8__cropped.png'),\n",
        " (40, 'training_dataset_cropped2\\\\15_10__cropped.png'),\n",
        " (41, 'training_dataset_cropped2\\\\15_11__cropped.png'),\n",
        " (42, 'training_dataset_cropped2\\\\15_1__cropped.png'),\n",
        " (43, 'training_dataset_cropped2\\\\15_3__cropped.png'),\n",
        " (44, 'training_dataset_cropped2\\\\15_5__cropped.png'),\n",
        " (45, 'training_dataset_cropped2\\\\15_7__cropped.png'),\n",
        " (46, 'training_dataset_cropped2\\\\15_8__cropped.png'),\n",
        " (47, 'training_dataset_cropped2\\\\15_9__cropped.png'),\n",
        " (48, 'training_dataset_cropped2\\\\1_10__cropped.png'),\n",
        " (49, 'training_dataset_cropped2\\\\1_11__cropped.png'),\n",
        " (50, 'training_dataset_cropped2\\\\1_2__cropped.png'),\n",
        " (51, 'training_dataset_cropped2\\\\1_4__cropped.png'),\n",
        " (52, 'training_dataset_cropped2\\\\1_5__cropped.png'),\n",
        " (53, 'training_dataset_cropped2\\\\1_6__cropped.png'),\n",
        " (54, 'training_dataset_cropped2\\\\1_7__cropped.png'),\n",
        " (55, 'training_dataset_cropped2\\\\1_9__cropped.png'),\n",
        " (56, 'training_dataset_cropped2\\\\2_11__cropped.png'),\n",
        " (57, 'training_dataset_cropped2\\\\2_1__cropped.png'),\n",
        " (58, 'training_dataset_cropped2\\\\2_2__cropped.png'),\n",
        " (59, 'training_dataset_cropped2\\\\2_3__cropped.png'),\n",
        " (60, 'training_dataset_cropped2\\\\2_5__cropped.png'),\n",
        " (60, 'training_dataset_cropped2\\\\2_6__cropped.png'),\n",
        " (62, 'training_dataset_cropped2\\\\2_8__cropped.png'),\n",
        " (63, 'training_dataset_cropped2\\\\2_9__cropped.png'),\n",
        " (64, 'training_dataset_cropped2\\\\3_11__cropped.png'),\n",
        " (65, 'training_dataset_cropped2\\\\3_1__cropped.png'),\n",
        " (66, 'training_dataset_cropped2\\\\3_3__cropped.png'),\n",
        " (67, 'training_dataset_cropped2\\\\3_4__cropped.png'),\n",
        " (68, 'training_dataset_cropped2\\\\3_5__cropped.png'),\n",
        " (68, 'training_dataset_cropped2\\\\3_6__cropped.png'),\n",
        " (70, 'training_dataset_cropped2\\\\3_7__cropped.png'),\n",
        " (71, 'training_dataset_cropped2\\\\3_9__cropped.png'),\n",
        " (72, 'training_dataset_cropped2\\\\4_10__cropped.png'),\n",
        " (73, 'training_dataset_cropped2\\\\4_11__cropped.png'),\n",
        " (74, 'training_dataset_cropped2\\\\4_1__cropped.png'),\n",
        " (75, 'training_dataset_cropped2\\\\4_3__cropped.png'),\n",
        " (76, 'training_dataset_cropped2\\\\4_5__cropped.png'),\n",
        " (77, 'training_dataset_cropped2\\\\4_6__cropped.png'),\n",
        " (77, 'training_dataset_cropped2\\\\4_8__cropped.png'),\n",
        " (79, 'training_dataset_cropped2\\\\4_9__cropped.png'),\n",
        " (80, 'training_dataset_cropped2\\\\5_10__cropped.png'),\n",
        " (81, 'training_dataset_cropped2\\\\5_1__cropped.png'),\n",
        " (82, 'training_dataset_cropped2\\\\5_3__cropped.png'),\n",
        " (83, 'training_dataset_cropped2\\\\5_5__cropped.png'),\n",
        " (84, 'training_dataset_cropped2\\\\5_6__cropped.png'),\n",
        " (85, 'training_dataset_cropped2\\\\5_7__cropped.png'),\n",
        " (86, 'training_dataset_cropped2\\\\5_8__cropped.png'),\n",
        " (87, 'training_dataset_cropped2\\\\5_9__cropped.png'),\n",
        " (88, 'training_dataset_cropped2\\\\6_11__cropped.png'),\n",
        " (89, 'training_dataset_cropped2\\\\6_2__cropped.png'),\n",
        " (90, 'training_dataset_cropped2\\\\6_3__cropped.png'),\n",
        " (91, 'training_dataset_cropped2\\\\6_4__cropped.png'),\n",
        " (92, 'training_dataset_cropped2\\\\6_5__cropped.png'),\n",
        " (93, 'training_dataset_cropped2\\\\6_7__cropped.png'),\n",
        " (94, 'training_dataset_cropped2\\\\6_8__cropped.png'),\n",
        " (95, 'training_dataset_cropped2\\\\6_9__cropped.png'),\n",
        " (96, 'training_dataset_cropped2\\\\7_10__cropped.png'),\n",
        " (97, 'training_dataset_cropped2\\\\7_11__cropped.png'),\n",
        " (98, 'training_dataset_cropped2\\\\7_1__cropped.png'),\n",
        " (99, 'training_dataset_cropped2\\\\7_2__cropped.png'),\n",
        " (100, 'training_dataset_cropped2\\\\7_3__cropped.png'),\n",
        " (101, 'training_dataset_cropped2\\\\7_4__cropped.png'),\n",
        " (102, 'training_dataset_cropped2\\\\7_6__cropped.png'),\n",
        " (103, 'training_dataset_cropped2\\\\7_8__cropped.png'),\n",
        " (104, 'training_dataset_cropped2\\\\8_11__cropped.png'),\n",
        " (105, 'training_dataset_cropped2\\\\8_1__cropped.png'),\n",
        " (106, 'training_dataset_cropped2\\\\8_2__cropped.png'),\n",
        " (107, 'training_dataset_cropped2\\\\8_4__cropped.png'),\n",
        " (108, 'training_dataset_cropped2\\\\8_5__cropped.png'),\n",
        " (109, 'training_dataset_cropped2\\\\8_7__cropped.png'),\n",
        " (110, 'training_dataset_cropped2\\\\8_8__cropped.png'),\n",
        " (111, 'training_dataset_cropped2\\\\8_9__cropped.png'),\n",
        " (112, 'training_dataset_cropped2\\\\9_10__cropped.png'),\n",
        " (113, 'training_dataset_cropped2\\\\9_11__cropped.png'),\n",
        " (114, 'training_dataset_cropped2\\\\9_2__cropped.png'),\n",
        " (115, 'training_dataset_cropped2\\\\9_5__cropped.png'),\n",
        " (115, 'training_dataset_cropped2\\\\9_6__cropped.png'),\n",
        " (117, 'training_dataset_cropped2\\\\9_7__cropped.png'),\n",
        " (118, 'training_dataset_cropped2\\\\9_8__cropped.png'),\n",
        " (119, 'training_dataset_cropped2\\\\9_9__cropped.png')]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np_fits = np.asarray(fits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files[np_fits.argmax()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "'doug_all/doug_test1\\\\doug_260396387_2_920499.png'"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l[:,0].max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "cannot perform reduce with flexible type",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-35dc92762271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Python27\\lib\\site-packages\\numpy\\core\\_methods.pyc\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     return um.maximum.reduce(a, axis=axis,\n\u001b[1;32m---> 17\u001b[1;33m                             out=out, keepdims=keepdims)\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxidx = l[:,0].argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "l[maxidx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "array(['-0.89193065198',\n",
        "       'doug_all/doug_test1\\\\doug_260396387_2_9289154.png'], \n",
        "      dtype='|S48')"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[(8, 2), (14, 4), (6, 10), (8, 15), (6, 15)]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits = np.asarray(fits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits[fits<20].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "(14,)"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fits"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_target.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "255"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_prob = clf.predict_proba(make_array(all_files[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "array([[  1.00000000e+00,   6.61838216e-46,   2.74725801e-38,\n",
        "          3.99919660e-42,   4.05870007e-41,   1.04088164e-26,\n",
        "          2.94991646e-38,   3.32041409e-33,   3.46870257e-61,\n",
        "          7.99012944e-40,   3.05495310e-34,   9.08117121e-33,\n",
        "          2.18125380e-53,   3.32008579e-33,   4.84449394e-34]])"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_target = make_array(all_files[20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# target_2d = np.asarray(np.atleast_2d(a_target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.dot(a_target - mean,scalings)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = np.dot(X, coeffs)+intercept"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "array([ -58.48831635,   27.94457604,  -59.93175328,  -37.66377309,\n",
        "       -169.86479197,   -5.86840073,  -11.48182364,  -24.32087449,\n",
        "        -28.47509457,  -22.70443046, -147.37975159,  -94.0779447 ,\n",
        "        -67.4654941 ,  -19.82000088,   -4.89433692])"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best = res.argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_best(target_face):\n",
      "    X = np.dot(target_face - mean,scalings)\n",
      "    res = np.dot(X, coeffs)+intercept\n",
      "    best = res.argmax()\n",
      "    return classes[best]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classes.take(best)\n",
      "print classes[best]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "2\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict_log_proba(make_array(all_files[5])).argmin()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "8"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_files[15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "'fullDataSetB_cropped2\\\\02_normal_cropped.png'"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "[(8, 2), (14, 4), (6, 10), (8, 15), (6, 15)]"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = clf.predict(make_array('fullDataSetB_cropped2/04_centerlight_cropped.png'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "int(predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_image = np.load('bin/eigenfaces/mean_image.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def image_grid(D,H,W,cols=10,scale=1):\n",
      "    \"\"\" display a grid of images\n",
      "        H,W: Height and width of the images\n",
      "        cols: number of columns = number of images in each row\n",
      "        scale: 1 to fill screen\n",
      "    \"\"\"\n",
      "    n = np.shape(D)[0]\n",
      "    rows = int(math.ceil((n+0.0)/cols))\n",
      "    fig = plt.figure(1,figsize=[scale*20.0/H*W,scale*20.0/cols*rows],dpi=300)\n",
      "    for i in range(n):\n",
      "        plt.subplot(rows,cols,i+1)\n",
      "        fig=plt.imshow(np.reshape(D[i,:],[H,W]), cmap = plt.get_cmap(\"gray\"))\n",
      "        plt.axis('off')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    }
   ],
   "metadata": {}
  }
 ]
}